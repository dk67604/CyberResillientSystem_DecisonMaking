{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494021\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"KDD\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "input_data=sc.textFile(\"/home/dharamendra/Downloads/kddcup.data_10_percent_corrected\", minPartitions=2).map(lambda line: line.encode(\"utf-8\"))\n",
    "print (input_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf. 280790\n",
      "neptune. 107201\n",
      "normal. 97278\n",
      "back. 2203\n",
      "satan. 1589\n",
      "ipsweep. 1247\n",
      "portsweep. 1040\n",
      "warezclient. 1020\n",
      "teardrop. 979\n",
      "pod. 264\n",
      "nmap. 231\n",
      "guess_passwd. 53\n",
      "buffer_overflow. 30\n",
      "land. 21\n",
      "warezmaster. 20\n",
      "imap. 12\n",
      "rootkit. 10\n",
      "loadmodule. 9\n",
      "ftp_write. 8\n",
      "multihop. 7\n",
      "phf. 4\n",
      "perl. 3\n",
      "spy. 2\n",
      "Counted in 5.305 seconds\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from time import time\n",
    "labels = input_data.map(lambda line: line.strip().split(\",\")[-1])\n",
    "t0 = time()\n",
    "label_counts = labels.countByValue()\n",
    "tt = time()-t0\n",
    "\n",
    "sorted_labels = OrderedDict(sorted(label_counts.items(), key=lambda t: t[1], reverse=True))\n",
    "for label, count in sorted_labels.items():\n",
    "    print (label, count)\n",
    "print (\"Counted in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration_r: string (nullable = true)\n",
      " |-- protocol_type_r: string (nullable = true)\n",
      " |-- service_r: string (nullable = true)\n",
      " |-- flag_r: string (nullable = true)\n",
      " |-- src_bytes_r: string (nullable = true)\n",
      " |-- dst_bytes_r: string (nullable = true)\n",
      " |-- land_r: string (nullable = true)\n",
      " |-- wrong_fragment_r: string (nullable = true)\n",
      " |-- urgent_r: string (nullable = true)\n",
      " |-- hot_r: string (nullable = true)\n",
      " |-- num_failed_logins_r: string (nullable = true)\n",
      " |-- logged_in_r: string (nullable = true)\n",
      " |-- num_compromised_r: string (nullable = true)\n",
      " |-- root_shell_r: string (nullable = true)\n",
      " |-- su_attempted_r: string (nullable = true)\n",
      " |-- num_root_r: string (nullable = true)\n",
      " |-- num_file_creations_r: string (nullable = true)\n",
      " |-- num_shells_r: string (nullable = true)\n",
      " |-- num_access_files_r: string (nullable = true)\n",
      " |-- num_outbound_cmds_r: string (nullable = true)\n",
      " |-- is_host_login_r: string (nullable = true)\n",
      " |-- is_guest_login_r: string (nullable = true)\n",
      " |-- count_r: string (nullable = true)\n",
      " |-- srv_count_r: string (nullable = true)\n",
      " |-- serror_rate_r: string (nullable = true)\n",
      " |-- srv_serror_rate_r: string (nullable = true)\n",
      " |-- rerror_rate_r: string (nullable = true)\n",
      " |-- srv_rerror_rate_r: string (nullable = true)\n",
      " |-- same_srv_rate_r: string (nullable = true)\n",
      " |-- diff_srv_rate_r: string (nullable = true)\n",
      " |-- srv_diff_host_rate_r: string (nullable = true)\n",
      " |-- dst_host_count_r: string (nullable = true)\n",
      " |-- dst_host_srv_count_r: string (nullable = true)\n",
      " |-- dst_host_same_srv_rate_r: string (nullable = true)\n",
      " |-- dst_host_diff_srv_rate_r: string (nullable = true)\n",
      " |-- dst_host_same_src_port_rate_r: string (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate_r: string (nullable = true)\n",
      " |-- dst_host_serror_rate_r: string (nullable = true)\n",
      " |-- dst_host_srv_serror_rate_r: string (nullable = true)\n",
      " |-- dst_host_rerror_rate_r: string (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate_r: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType,DoubleType\n",
    "def replacetab(line):\n",
    "    data = line.split(',')\n",
    "    return data\n",
    "\n",
    "\n",
    "process_data = input_data.map(lambda line: replacetab(line))\n",
    "\n",
    "\n",
    "fields = [StructField(\"duration_r\", StringType(), True),\n",
    "              StructField(\"protocol_type_r\", StringType(), True), StructField(\"service_r\", StringType(), True),\n",
    "              StructField(\"flag_r\", StringType(), True),\n",
    "              StructField(\"src_bytes_r\", StringType(), True), StructField(\"dst_bytes_r\", StringType(), True),\n",
    "              StructField(\"land_r\", StringType(), True),\n",
    "              StructField(\"wrong_fragment_r\", StringType(), True), StructField(\"urgent_r\", StringType(), True),\n",
    "              StructField(\"hot_r\", StringType(), True),\n",
    "              StructField(\"num_failed_logins_r\", StringType(), True),StructField(\"logged_in_r\", StringType(), True), \n",
    "              StructField(\"num_compromised_r\", StringType(), True),\n",
    "              StructField(\"root_shell_r\", StringType(), True),StructField(\"su_attempted_r\", StringType(), True),\n",
    "              StructField(\"num_root_r\", StringType(), True),\n",
    "              StructField(\"num_file_creations_r\", StringType(), True),\n",
    "              StructField(\"num_shells_r\", StringType(), True),\n",
    "              StructField(\"num_access_files_r\", StringType(), True), StructField(\"num_outbound_cmds_r\", StringType(), True),\n",
    "              StructField(\"is_host_login_r\", StringType(), True),\n",
    "              StructField(\"is_guest_login_r\", StringType(), True), StructField(\"count_r\", StringType(), True),\n",
    "              StructField(\"srv_count_r\", StringType(), True),\n",
    "              StructField(\"serror_rate_r\", StringType(), True), StructField(\"srv_serror_rate_r\", StringType(), True),\n",
    "              StructField(\"rerror_rate_r\", StringType(), True),\n",
    "              StructField(\"srv_rerror_rate_r\", StringType(), True), StructField(\"same_srv_rate_r\", StringType(), True),\n",
    "              StructField(\"diff_srv_rate_r\", StringType(), True),\n",
    "              StructField(\"srv_diff_host_rate_r\", StringType(), True), StructField(\"dst_host_count_r\", StringType(), True),\n",
    "              StructField(\"dst_host_srv_count_r\", StringType(), True),\n",
    "              StructField(\"dst_host_same_srv_rate_r\", StringType(), True), StructField(\"dst_host_diff_srv_rate_r\", StringType(), True),\n",
    "              StructField(\"dst_host_same_src_port_rate_r\", StringType(), True),\n",
    "              StructField(\"dst_host_srv_diff_host_rate_r\", StringType(), True), StructField(\"dst_host_serror_rate_r\", StringType(), True),\n",
    "              StructField(\"dst_host_srv_serror_rate_r\", StringType(), True),\n",
    "              StructField(\"dst_host_rerror_rate_r\", StringType(), True), StructField(\"dst_host_srv_rerror_rate_r\", StringType(), True),\n",
    "              StructField(\"label\", StringType(), True)]\n",
    "schema = StructType(fields)\n",
    "input_dataframe = spark.createDataFrame(process_data, schema)\n",
    "input_dataframe.printSchema()\n",
    "\n",
    "#input_dataframe.describe('src_bytes','wrong_fragment').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------+-------+--------+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+\n",
      "|protocol_type_r|service_r|flag_r|  label|duration|src_bytes|dst_bytes|land|wrong_fragment|urgent|hot|num_failed_logins|logged_in|num_compromised|root_shell|su_attempted|num_root|num_file_creations|num_shells|num_access_files|num_outbound_cmds|is_host_login|is_guest_login|count|srv_count|serror_rate|srv_serror_rate|rerror_rate|srv_rerror_rate|same_srv_rate|diff_srv_rate|srv_diff_host_rate|dst_host_count|dst_host_srv_count|dst_host_same_srv_rate|dst_host_diff_srv_rate|dst_host_same_src_port_rate|dst_host_srv_diff_host_rate|dst_host_serror_rate|dst_host_srv_serror_rate|dst_host_rerror_rate|dst_host_srv_rerror_rate|\n",
      "+---------------+---------+------+-------+--------+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+\n",
      "|            tcp|     http|    SF|normal.|       0|      181|     5450|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    8|        8|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|             9|                 9|                   1.0|                   0.0|                       0.11|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|\n",
      "|            tcp|     http|    SF|normal.|       0|      239|      486|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    8|        8|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|            19|                19|                   1.0|                   0.0|                       0.05|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|\n",
      "+---------------+---------+------+-------+--------+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changedTypedf = input_dataframe.withColumn(\"duration\", input_dataframe[\"duration_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"src_bytes\", input_dataframe[\"src_bytes_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"dst_bytes\", input_dataframe[\"dst_bytes_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"land\", input_dataframe[\"land_r\"].cast(\"integer\")). \\\n",
    "        withColumn(\"wrong_fragment\", input_dataframe[\"wrong_fragment_r\"].cast(\"integer\")). \\\n",
    "        withColumn(\"urgent\", input_dataframe[\"urgent_r\"].cast(\"integer\")). \\\n",
    "        withColumn(\"hot\", input_dataframe[\"hot_r\"].cast(\"integer\")). \\\n",
    "        withColumn(\"num_failed_logins\", input_dataframe[\"num_failed_logins_r\"].cast(\"integer\")). \\\n",
    "        withColumn(\"logged_in\", input_dataframe[\"logged_in_r\"].cast(\"integer\")). \\\n",
    "        withColumn(\"num_compromised\", input_dataframe[\"num_compromised_r\"].cast(\"integer\")). \\\n",
    "        withColumn(\"root_shell\", input_dataframe[\"root_shell_r\"].cast(\"integer\")). \\\n",
    "        withColumn(\"su_attempted\", input_dataframe[\"su_attempted_r\"].cast(\"integer\")). \\\n",
    "        withColumn(\"num_root\", input_dataframe[\"num_root_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"num_file_creations\", input_dataframe[\"num_file_creations_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"num_shells\", input_dataframe[\"num_shells_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"num_access_files\", input_dataframe[\"num_access_files_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"num_outbound_cmds\", input_dataframe[\"num_outbound_cmds_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"is_host_login\", input_dataframe[\"is_host_login_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"is_guest_login\", input_dataframe[\"is_guest_login_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"count\", input_dataframe[\"count_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"srv_count\", input_dataframe[\"srv_count_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"serror_rate\", input_dataframe[\"serror_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"srv_serror_rate\", input_dataframe[\"srv_serror_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"rerror_rate\", input_dataframe[\"rerror_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"srv_rerror_rate\", input_dataframe[\"srv_rerror_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"same_srv_rate\", input_dataframe[\"same_srv_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"diff_srv_rate\", input_dataframe[\"diff_srv_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"srv_diff_host_rate\", input_dataframe[\"srv_diff_host_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"dst_host_count\", input_dataframe[\"dst_host_count_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"dst_host_srv_count\", input_dataframe[\"dst_host_srv_count_r\"].cast(\"integer\")).\\\n",
    "        withColumn(\"dst_host_same_srv_rate\", input_dataframe[\"dst_host_same_srv_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"dst_host_diff_srv_rate\", input_dataframe[\"dst_host_diff_srv_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"dst_host_same_src_port_rate\", input_dataframe[\"dst_host_same_src_port_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"dst_host_srv_diff_host_rate\", input_dataframe[\"dst_host_srv_diff_host_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"dst_host_serror_rate\", input_dataframe[\"dst_host_serror_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"dst_host_srv_serror_rate\", input_dataframe[\"dst_host_srv_serror_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"dst_host_rerror_rate\", input_dataframe[\"dst_host_rerror_rate_r\"].cast(\"double\")).\\\n",
    "        withColumn(\"dst_host_srv_rerror_rate\", input_dataframe[\"dst_host_srv_rerror_rate_r\"].cast(\"double\"))\n",
    "\n",
    "input_filter = changedTypedf.drop(\"duration_r\").drop(\"src_bytes_r\").drop(\"dst_bytes_r\").drop(\"land_r\").drop(\"wrong_fragment_r\").drop(\"urgent_r\"). \\\n",
    "        drop(\"logged_in_r\").drop(\"num_compromised_r\").drop(\"rerror_rate_r\").drop(\"srv_rerror_rate_r\").\\\n",
    "        drop(\"dst_host_rerror_rate_r\").drop(\"dst_host_srv_rerror_rate_r\").\\\n",
    "        drop(\"hot_r\").drop(\"num_failed_logins_r\").drop(\"root_shell_r\").drop(\"su_attempted_r\").drop(\"num_root_r\").drop(\"num_file_creations_r\"). \\\n",
    "        drop(\"num_shells_r\").drop(\"num_access_files_r\").drop(\"num_outbound_cmds_r\").drop(\"is_host_login_r\").drop(\"is_guest_login_r\").\\\n",
    "        drop(\"count_r\").drop(\"srv_count_r\").drop(\"serror_rate_r\").drop(\"srv_serror_rate_r\").drop(\"same_srv_rate_r\").\\\n",
    "        drop(\"diff_srv_rate_r\").drop(\"srv_diff_host_rate_r\").drop(\"dst_host_count_r\").drop(\"dst_host_srv_count_r\").drop(\"dst_host_same_srv_rate_r\").\\\n",
    "        drop(\"dst_host_diff_srv_rate_r\").drop(\"dst_host_same_src_port_rate_r\").drop(\"dst_host_srv_diff_host_rate_r\").drop(\"dst_host_serror_rate_r\").drop(\"dst_host_srv_serror_rate_r\")\n",
    "        \n",
    "input_filter.show(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "categorical_col=[\"protocol_type_r\",\"service_r\",\"flag_r\"]\n",
    "                                                                                                                                                                                \n",
    "stringIndexer_cat = [StringIndexer(inputCol=column, outputCol=column + \"_i\").fit(input_filter) for column in categorical_col]\n",
    "pipeline = Pipeline(stages=stringIndexer_cat)\n",
    "cat_model = pipeline.fit(input_filter)\n",
    "cat_indexed = cat_model.transform(input_filter).drop(\"protocol_type_r\").drop(\"service_r\").drop(\"flag_r\")\n",
    "\n",
    "\n",
    "\n",
    "# scaler = MinMaxScaler(inputCol=\"duration\", outputCol=\"duration_n\")\n",
    "# scalerModel = scaler.fit(input_filter)\n",
    "# scaledData = scalerModel.transform(input_filter)\n",
    "# input_filter.describe('duration_n').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- src_bytes: integer (nullable = true)\n",
      " |-- dst_bytes: integer (nullable = true)\n",
      " |-- land: integer (nullable = true)\n",
      " |-- wrong_fragment: integer (nullable = true)\n",
      " |-- urgent: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- num_failed_logins: integer (nullable = true)\n",
      " |-- logged_in: integer (nullable = true)\n",
      " |-- num_compromised: integer (nullable = true)\n",
      " |-- root_shell: integer (nullable = true)\n",
      " |-- su_attempted: integer (nullable = true)\n",
      " |-- num_root: integer (nullable = true)\n",
      " |-- num_file_creations: integer (nullable = true)\n",
      " |-- num_shells: integer (nullable = true)\n",
      " |-- num_access_files: integer (nullable = true)\n",
      " |-- num_outbound_cmds: integer (nullable = true)\n",
      " |-- is_host_login: integer (nullable = true)\n",
      " |-- is_guest_login: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- srv_count: integer (nullable = true)\n",
      " |-- serror_rate: double (nullable = true)\n",
      " |-- srv_serror_rate: double (nullable = true)\n",
      " |-- rerror_rate: double (nullable = true)\n",
      " |-- srv_rerror_rate: double (nullable = true)\n",
      " |-- same_srv_rate: double (nullable = true)\n",
      " |-- diff_srv_rate: double (nullable = true)\n",
      " |-- srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_count: integer (nullable = true)\n",
      " |-- dst_host_srv_count: integer (nullable = true)\n",
      " |-- dst_host_same_srv_rate: double (nullable = true)\n",
      " |-- dst_host_diff_srv_rate: double (nullable = true)\n",
      " |-- dst_host_same_src_port_rate: double (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_serror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_serror_rate: double (nullable = true)\n",
      " |-- dst_host_rerror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate: double (nullable = true)\n",
      " |-- protocol_type_r_i: double (nullable = true)\n",
      " |-- service_r_i: double (nullable = true)\n",
      " |-- flag_r_i: double (nullable = true)\n",
      " |-- label_r: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def replacedot(label_i):\n",
    "    return label_i.replace(\".\",\"\")\n",
    "\n",
    "def custom_udf(x):\n",
    "    y = udf(replacedot, StringType())\n",
    "    return y(x)\n",
    "\n",
    "\n",
    "input_df=cat_indexed.withColumn(\"label_r\",custom_udf(cat_indexed.label)).drop(\"label\")\n",
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|label_r|            features|\n",
      "+-------+--------------------+\n",
      "| normal|(41,[1,2,8,19,20,...|\n",
      "| normal|(41,[1,2,8,19,20,...|\n",
      "+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    " \n",
    "col_names = [\"duration\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"protocol_type_r_i\",\"service_r_i\",\"flag_r_i\"]\n",
    "assembler = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "output = assembler.transform(input_df)\n",
    "assembledData = output.select([column for column in output.columns if column not in col_names])\n",
    "assembledData.show(2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+\n",
      "|label_r|      scaledFeatures|label|\n",
      "+-------+--------------------+-----+\n",
      "| normal|[0.0,2.6104176374...|  2.0|\n",
      "| normal|[0.0,3.4469050571...|  2.0|\n",
      "| normal|[0.0,3.3892162695...|  2.0|\n",
      "| normal|[0.0,3.1584611192...|  2.0|\n",
      "| normal|[0.0,3.1296167255...|  2.0|\n",
      "| normal|[0.0,3.1296167255...|  2.0|\n",
      "| normal|[0.0,3.0575057410...|  2.0|\n",
      "| normal|[0.0,2.2931293057...|  2.0|\n",
      "| normal|[0.0,3.0286613472...|  2.0|\n",
      "| normal|[0.0,3.0575057410...|  2.0|\n",
      "| normal|[0.0,3.0286613472...|  2.0|\n",
      "| normal|[0.0,2.5527288498...|  2.0|\n",
      "| normal|[0.0,3.2017277099...|  2.0|\n",
      "| normal|[0.0,3.6920824042...|  2.0|\n",
      "| normal|[0.0,3.4757494509...|  2.0|\n",
      "| normal|[0.0,3.7497711918...|  2.0|\n",
      "| normal|[0.0,3.4757494509...|  2.0|\n",
      "| normal|[0.0,3.7065046011...|  2.0|\n",
      "| normal|[0.0,3.3603718757...|  2.0|\n",
      "| normal|[0.0,3.3603718757...|  2.0|\n",
      "| normal|[0.0,3.6920824042...|  2.0|\n",
      "| normal|[0.0,3.3747940726...|  2.0|\n",
      "| normal|[0.0,3.4757494509...|  2.0|\n",
      "| normal|[0.0,3.4469050571...|  2.0|\n",
      "| normal|[0.0,3.5334382384...|  2.0|\n",
      "| normal|[0.0,3.5767048291...|  2.0|\n",
      "| normal|[0.0,5.1054576996...|  2.0|\n",
      "| normal|[0.0,2.7834840001...|  2.0|\n",
      "| normal|[0.0,3.0863501348...|  2.0|\n",
      "| normal|[0.0,3.0575057410...|  2.0|\n",
      "| normal|[0.0,3.1007723317...|  2.0|\n",
      "| normal|[0.0,3.1296167255...|  2.0|\n",
      "| normal|[0.0,2.9565503628...|  2.0|\n",
      "| normal|[0.0,2.2354405182...|  2.0|\n",
      "| normal|[0.0,2.9132837721...|  2.0|\n",
      "| normal|[0.0,3.3892162695...|  2.0|\n",
      "| normal|[0.0,3.7353489949...|  2.0|\n",
      "| normal|[0.0,4.3410812644...|  2.0|\n",
      "| normal|[0.0,4.6439473991...|  2.0|\n",
      "| normal|[0.0,5.3362128499...|  2.0|\n",
      "| normal|[0.0,5.3362128499...|  2.0|\n",
      "| normal|[0.0,2.4806178653...|  2.0|\n",
      "| normal|[0.0,3.8074599794...|  2.0|\n",
      "| normal|[0.0,3.6776602073...|  2.0|\n",
      "| normal|[0.0,3.9516819483...|  2.0|\n",
      "| normal|[0.0,4.5141476271...|  2.0|\n",
      "| normal|[0.0,2.0912185492...|  2.0|\n",
      "| normal|[0.0,4.1824370985...|  2.0|\n",
      "| normal|[0.0,4.4564588395...|  2.0|\n",
      "| normal|[0.0,4.5718364146...|  2.0|\n",
      "| normal|[0.0,4.3266590675...|  2.0|\n",
      "| normal|[0.0,4.4276144457...|  2.0|\n",
      "| normal|[0.0,4.4564588395...|  2.0|\n",
      "| normal|[0.0,4.4131922488...|  2.0|\n",
      "| normal|[0.0,3.4180606633...|  2.0|\n",
      "| normal|[0.0,3.4180606633...|  2.0|\n",
      "| normal|[0.0,3.4469050571...|  2.0|\n",
      "| normal|[0.0,2.3075515026...|  2.0|\n",
      "| normal|[0.0,3.3315274819...|  2.0|\n",
      "| normal|[0.0,3.2738386944...|  2.0|\n",
      "| normal|[0.0,3.3459496788...|  2.0|\n",
      "| normal|[0.0,3.3603718757...|  2.0|\n",
      "| normal|[0.0,3.4036384664...|  2.0|\n",
      "| normal|[0.0,4.6439473991...|  2.0|\n",
      "| normal|[0.0,4.7160583835...|  2.0|\n",
      "| normal|[0.0,3.2449943006...|  2.0|\n",
      "| normal|[0.0,3.2305721037...|  2.0|\n",
      "| normal|[0.0,3.2305721037...|  2.0|\n",
      "| normal|[0.0,3.3171052850...|  2.0|\n",
      "| normal|[0.0,3.4469050571...|  2.0|\n",
      "| normal|[0.0,3.4613272540...|  2.0|\n",
      "| normal|[0.0,3.4180606633...|  2.0|\n",
      "| normal|[0.0,3.3026830882...|  2.0|\n",
      "| normal|[0.0,2.3652402902...|  2.0|\n",
      "| normal|[0.0,3.4901716477...|  2.0|\n",
      "| normal|[0.0,3.4324828602...|  2.0|\n",
      "| normal|[0.0,3.4324828602...|  2.0|\n",
      "| normal|[0.0,2.9998169534...|  2.0|\n",
      "| normal|[0.0,3.0719279379...|  2.0|\n",
      "| normal|[0.0,4.2833924768...|  2.0|\n",
      "| normal|[0.0,4.3555034612...|  2.0|\n",
      "| normal|[0.0,4.4708810364...|  2.0|\n",
      "| normal|[0.0,4.5285698239...|  2.0|\n",
      "| normal|[0.0,4.4564588395...|  2.0|\n",
      "| normal|[0.0,4.4276144457...|  2.0|\n",
      "| normal|[0.0,4.4708810364...|  2.0|\n",
      "| normal|[0.0,2.6969508187...|  2.0|\n",
      "| normal|[0.0,2.8988615752...|  2.0|\n",
      "| normal|[0.0,2.2354405182...|  2.0|\n",
      "| normal|[0.0,3.2017277099...|  2.0|\n",
      "| normal|[0.0,3.1584611192...|  2.0|\n",
      "| normal|[0.0,3.1728833161...|  2.0|\n",
      "| normal|[0.0,3.3171052850...|  2.0|\n",
      "| normal|[0.0,3.1873055130...|  2.0|\n",
      "| normal|[0.0,4.7449027773...|  2.0|\n",
      "| normal|[0.0,4.8602803525...|  2.0|\n",
      "| normal|[0.0,4.2257036892...|  2.0|\n",
      "| normal|[0.0,2.4661956684...|  2.0|\n",
      "| normal|[0.0,2.5671510467...|  2.0|\n",
      "| normal|[0.0,4.7737471711...|  2.0|\n",
      "+-------+--------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(assembledData)\n",
    "\n",
    "scaledData = scalerModel.transform(assembledData).drop(\"features\")\n",
    "labelIndexer = StringIndexer(inputCol=\"label_r\", outputCol=\"label\").fit(scaledData)\n",
    "\n",
    "train_df=labelIndexer.transform(scaledData)\n",
    "#train_df.show(100)\n",
    "\n",
    "train=train_df.selectExpr(\"scaledFeatures as features\" ,\"label\")\n",
    "#train.persist(StorageLevel(True, True, False, False, 1))\n",
    "#train.write.parquet(\"final_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,2.6104176374...|  2.0|\n",
      "|[0.0,3.4469050571...|  2.0|\n",
      "+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.parquet(\"final_data.parquet\")\n",
    "train_df.show(2)\n",
    "#train_df.persist(StorageLevel(True, True, False, False, 1))\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "df_test, df_train = train_df.randomSplit([0.3, 0.7])\n",
    "rfc = RandomForestClassifier(maxDepth=8, maxBins=2400000, numTrees=128,impurity=\"gini\")\n",
    "rfc_model = rfc.fit(train_df)\n",
    "rfc_model.save(\"rfc_model\")\n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[0.0,0.0,0.0,0.0,...|  6.0|[0.0,2.2195269323...|[0.0,0.0693602166...|       6.0|\n",
      "|[0.0,0.0,0.0,0.0,...|  2.0|[0.15643338919068...|[0.00488854341220...|       2.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "0.998682414627\n",
      "0.998569922522\n",
      "0.998682414627\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "train_df = spark.read.parquet(\"final_data.parquet\")\n",
    "df_test, df_train = train_df.randomSplit([0.3, 0.7])\n",
    "rfc = RandomForestClassifier(maxDepth=8, maxBins=12000, numTrees=32,impurity=\"gini\")\n",
    "rfc_model = rfc.fit(df_train)\n",
    "predictions = rfc_model.transform(df_test)\n",
    "predictions.registerTempTable('Predictions')\n",
    "predictions.show(2)\n",
    "\n",
    "evaluator_acc = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "print (accuracy)\n",
    "evaluator_acc = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedPrecision\")\n",
    "precison=evaluator_acc.evaluate(predictions)\n",
    "print (precison)\n",
    "evaluator_acc = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "recall=evaluator_acc.evaluate(predictions)\n",
    "print (recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|TruePositive|\n",
      "+------------+\n",
      "|      118832|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.createOrReplaceTempView(\"predictionTable\")\n",
    "test=spark.sql(\"SELECT count(*) as TruePositive from predictionTable a1 where a1.label!=2.0\")\n",
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
